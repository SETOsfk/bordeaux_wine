{"metadata":{"kernelspec":{"name":"ir","display_name":"R","language":"R"},"language_info":{"name":"R","codemirror_mode":"r","pygments_lexer":"r","mimetype":"text/x-r-source","file_extension":".r","version":"4.0.5"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8062196,"sourceType":"datasetVersion","datasetId":4755862}],"dockerImageVersionId":30618,"isInternetEnabled":true,"language":"r","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/sertanafak/bordeuax-wine-classification-93-5-roc?scriptVersionId=182253513\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"#Necessary packages.\nlibrary(caret)\nlibrary(randomForest)\nlibrary(dplyr)\nlibrary(tidyverse)\nlibrary(ROSE)\nlibrary(ada)\nlibrary(pROC)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Creating the dataset\nwine<-read_csv(\"/kaggle/input/21st-century-bordeaux-wine-dataset/BordeauxWines.csv\")\n#To seperate first four columns so we can do the transformation.\nmerge<-wine[,1:4]\nwine<-wine[,-1:-4]\n#Transforming double's to factors\nwine<- wine %>% mutate_if(is.double,as.factor)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"First we need to check data types for better analysis. All of the columns except first four column coded as double but they need to be two levelled factors.Some factors only has one level which is \"0\". This creates zero variance so that is not gonna effect the model because of that, we will remove them. What we are looking for is what makes a wine good. So we will group scores as 1 and 0 which means 90+ score wines and 90- score wines. To do this we will create a column called \"Diagnose\"","metadata":{}},{"cell_type":"code","source":"#To remove which only has one level.\ncols_to_remove <- sapply(wine, function(x) is.factor(x) && length(levels(x)) == 1)\nwine_main<-wine[,!cols_to_remove]\n\n#Creating a column called \"Diagnose\" for classes                         \nDiagnose <- c(1:14349)\nwine_main<-data.frame(wine_main,Diagnose,merge)\nwine_main$Diagnose\nwine_main <- wine_main %>%\n  mutate(Diagnose = case_when(\n    wine_main$Score >=90 ~\"X1\",\n  TRUE ~\"X0\")) %>% \n  select(Diagnose,Score,Name,Year,Price,everything())\nwine_main$Diagnose<-as.factor(wine_main$Diagnose)\nwine_main<- wine_main %>% select(-Year,-Name,-Price,-Score)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Dataset has lots of columns which might cause an overfitting problem or lots of computational time. We need to reduce column size which known as \"dimension reduction\". \nWe will use RFI for this. Basicly it's gonna find the most effective columns, we can use other methods such as NZV (near zero variance). I've tried NZV it gives the same result with RFI.","metadata":{}},{"cell_type":"code","source":"#Random forest for feature selection.\nrf_model <- randomForest(Diagnose ~ ., data = wine_main, importance = TRUE, ntree = 100)\nimportance_scores <- importance(rf_model, type = 1)\n\nif (is.matrix(importance_scores)) {\n  importance_values <- importance_scores[, 1] # Change 1 if another column is needed\n} else {\n  importance_values <- importance_scores\n}\n\n\nsorted_importance <- sort(importance_values, decreasing = TRUE)\nimportant_features <- names(sorted_importance)[1:50]\n\nX_reduced <- wine_main[,c(important_features)]\nDiagnose<-wine_main$Diagnose\nX_reduced<- data.frame(Diagnose,X_reduced)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We'll split dataset %70 training and %30 test. But with createDataPartion function we will easily split dataset according to class ratio in \"Diagnose\" column which is our key column. So basicly, in main dataset diagnose column has two levels which is 1 and 0. Ratio between em is 0.7 \"1\" class and 0.3 \"1\" class so we will keep this ratio in our training and test datasets too.","metadata":{}},{"cell_type":"code","source":"trainIndex <- createDataPartition(X_reduced$Diagnose, p = .7, \n                                  list = FALSE, \n                                  times = 1)\ntrain_data <- X_reduced[ trainIndex,]\ntest_data  <- X_reduced[-trainIndex,]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have unbalanced classes which may cause low sensitiviy. To solve this we will use \"both\" method from \"ROSE\" package.","metadata":{}},{"cell_type":"code","source":"barplot(prop.table(table(X_reduced$Diagnose)),\n        col = rainbow(2),\n        ylim = c(0, 0.7),\n        main = \"Class Ratios\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bothq <- ovun.sample(Diagnose~., data = train_data, method = \"both\")\ntrain_data<-bothq$data\n\nbarplot(prop.table(table( train_data$Diagnose)),\n        col = rainbow(2),\n        ylim = c(0, 0.7),\n        main = \"Class Ratios\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"What \"both\" does is; method combines these two approaches by simultaneously over-sampling the minority class and under-sampling the majority class. This balanced approach aims to mitigate the disadvantages of both methods when used independently.","metadata":{}},{"cell_type":"markdown","source":"For better results we will use trainControl.","metadata":{}},{"cell_type":"code","source":"train_control <- trainControl(\n  method = \"cv\",       # Cross-validation\n  number = 5,          # 5-fold cross-validation\n  classProbs = TRUE,   # Compute class probabilities\n  summaryFunction = twoClassSummary # Summary function for classification\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Logistic Regression\nlogistic_model <- train(\n  Diagnose ~ ., data = train_data, \n  method = \"glm\", \n  family = binomial, \n  trControl = train_control, \n  metric = \"ROC\"\n)\n# Random Forest\nrf_model <- train(\n  Diagnose ~ ., data = train_data, \n  method = \"rf\", \n  trControl = train_control, \n  metric = \"ROC\"\n)\n\n# GBM\ngbm_model <- train(\n  Diagnose ~ ., data = train_data, \n  method = \"gbm\", \n  trControl = train_control, \n  verbose = FALSE,\n  metric = \"ROC\"\n)\n\n# SVM\nsvm_model <- train(\n  Diagnose ~ ., data = train_data, \n  method = \"svmRadial\", \n  trControl = train_control, \n  metric = \"ROC\"\n)\n\n# AdaBoost\nada_model <- train(\n  Diagnose ~ ., data = train_data, \n  method = \"ada\", \n  trControl = train_control, \n  metric = \"ROC\"\n)\n\n\n# CART\ncart_model <- train(\n  Diagnose ~ ., data = train_data, \n  method = \"rpart\", \n  trControl = train_control, \n  metric = \"ROC\"\n)\n\n# LDA\nlda_model <- train(\n  Diagnose ~ ., data = train_data, \n  method = \"lda\", \n  trControl = train_control, \n  metric = \"ROC\"\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 5. Resample Comparison\nresults <- resamples(list(\n  LOG = logistic_model, \n  RF = rf_model, \n  GBM = gbm_model, \n  SVM = svm_model,\n  ADAB = ada_model, \n  CART = cart_model,\n  LDA = lda_model\n))\n\n# 6. Summary of the results\nsummary(results)\ndotplot(results)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Results are fine but they may lead to misinformation, because we used \"both\" function to create and eliminate some data. To see the real results we will use test dataset.","metadata":{}},{"cell_type":"code","source":"# 7. Evaluate Models\n\n#SVM\nsvm_preds <- predict(svm_model, newdata = test_data)\nsvm_probs <- predict(svm_model, newdata = test_data, type = \"prob\")\nsvm_roc <- roc(response = test_data$Diagnose, predictor = svm_probs[,2])\nprint(auc(svm_roc))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Random Forest\nrf_preds <- predict(rf_model, newdata = test_data)\nrf_probs <- predict(rf_model, newdata = test_data, type = \"prob\")\nrf_roc <- roc(response = test_data$Diagnose, predictor = rf_probs[,2])\nprint(auc(rf_roc))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#GBM\ngbm_preds <- predict(gbm_model, newdata = test_data)\ngbm_probs <- predict(gbm_model, newdata = test_data, type = \"prob\")\ngbm_roc <- roc(response = test_data$Diagnose, predictor = gbm_probs[,2])\nprint(auc(gbm_roc))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Confusion Matrices\nprint(confusionMatrix(rf_preds, test_data$Diagnose))\nprint(confusionMatrix(svm_preds, test_data$Diagnose))\nprint(confusionMatrix(gbm_preds, test_data$Diagnose))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"What we should look is the \"balanced accuracy\". Because dataset has unbalanced classes. Which may cause models to provide good accuracy but model will correctly predict one class significantly better\nthan the other class.","metadata":{}},{"cell_type":"code","source":"varimp_rf <- varImp(rf_model)\nplot(varimp_rf, main=\"Most valuable columns for rf\")\n\nvarimp_svm <- varImp(svm_model)\nplot(varimp_svm, main=\"Most valuable columns for svm\")","metadata":{},"execution_count":null,"outputs":[]}]}